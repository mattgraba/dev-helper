# Note Canvas (9/17)

### OPEN Items (Carry Forward)
- **Schema validation for AI responses (Zod integration)**
    ‚Üí Priority: finish this first to lock reliability before scaling.
- **Batch audit log support**
    ‚Üí Next layer once validation is in place.
- **History integration for compliance analyses**
    ‚Üí Useful for traceability & repeatability, but after validation
- **Decide timing of moving logic from Dev Helper ‚Üí Cannabis-Compliance backend**
    ‚Üí A stregic call once Dev Helper prototypes feel stable.

### What was Closed Yesterday
- Validated Dev Helper compliance CLI end-to-end with sample log.
- Fixed env + lazy init issues.
- Positioned **reliability > scaling** as the immediate focus.

### Today's Priorities
1. **Schema Validation (Zod)**
    - Define a `ComplianceAnalysisResponseSchema` to validate all AI outputs.
    - Enforce it in `handleWithContext` or wherever responses are parsed.
    - Log/throw descriptive errors if validation fails.
    - Bonus: create a `tests/sample-invalid-response.json` to test failure cases.
2. **Scaffold Cannabis-Compliance Service/Controller**
    - Continue where left off with **skeleton files** (`auditAutomationService.js`, `auditAutomationController.js`).
    - Service: placeholder methods (`analyzeAudit`, `batchAnalyze`, etc,) w/ TODOs.
    - Controller: wire routes, basic request/response scaffolding.

### Rolling Plan (Next 3 Days)
- **Day 1 (Today)**: Zod schema + stub out service/controller files.
- **Day 2**: Wire batch audit log support in Dev Helper (using validated schema).
- **Day 3**: Explore history integration + evaluate timing for backend migration.

---

## Implement: Zod schema + stub out service/controller filse.

Since **Dev Helper = sandbox/lab** we'll colocate everything that belongs to our compliance prototype in one place.
```cpp
commands/
  compliance/
    index.ts                // entry point for compliance CLI command(s)
    schemas/
      complianceAnalysisResponse.ts   // Zod schemas for AI responses
    utils/
      parseComplianceResponse.ts      // helpers for validation + error handling
    __tests__/
      complianceAnalysisResponse.test.ts // schema validation unit tests
```
#### **Why this structure works:**
- **Schemas folder**: keeps validation definitions isolated, reusable, and easy to migrate later in Cannabis-Compliance.
- **Utils folder**: holds parsing/validation helpers ‚Üí avoids bloating the command logic.
- **Tests folder**: gives us an immediate harness to check schema validity on sample JSON.
- **index.ts**: stays clean and focusd on wiring commands, not schema details.
When we move logic into Cannabis-Compliance later, we'll already have a neat package of schema + parser + tests ready to migrate.

### Zod Schema for Compliance Analysis Response
**`commands/compliance/schemas/complianceAnalysisResponse.ts`**
```ts
import { z } from "zod"

export const ComplianceIssueSchema = z.object({
  id: z.string().uuid().optional(),       // optional if AI doesn‚Äôt always assign
  message: z.string(),                    // description of the compliance issue
  severity: z.enum(["low", "medium", "high", "critical"]),
  regulation: z.string().optional(),      // e.g., "OSHA 1910.1200" or "METRC"
  suggestion: z.string().optional(),      // AI-generated recommendation
})

export const ComplianceAnalysisResponseSchema = z.object({
  auditId: z.string(),                    // unique audit log ID
  timestamp: z.string().datetime(),       // ISO 8601 timestamp
  analyzedBy: z.string().default("AI"),   // ‚ÄúAI‚Äù, ‚Äúsystem‚Äù, or user id
  summary: z.string(),                    // high-level summary of the analysis
  issues: z.array(ComplianceIssueSchema).nonempty(), // at least one issue
  metadata: z
    .object({
      sourceFile: z.string().optional(),  // file or log analyzed
      tokensUsed: z.number().optional(),  // optional tracking
    })
    .optional(),
})

// Type inference (for TS users)
export type ComplianceAnalysisResponse = z.infer<typeof ComplianceAnalysisResponseSchema>
export type ComplianceIssue = z.infer<typeof ComplianceIssueSchema>
```
#### **1. What's in the Schema (and Why)**
Our **Zod schema** is the contract that every AI response must meet before it flows deeper into our system.
- `ComplianceIssueSchema`
Defines one individual compliance issue. Each issue has:
    - `message`: the human-readable description of the problem (required).
    - `severity`: standardized enum (`low`‚Üí`critical`).
    - `regulation`: optional string to tie the issue to a rule (e.g. OSHA, METRC).
    - `suggestion`: optional fix or recommendation.
- `ComplianceAnalysisResponseSchema`
Defines the overall shape of an analysis:
    - `auditId`: unique ID linking to our audit log.
    - `timestamp`: when analysis occurred.
    - `analyzedBy`: who/what did the analysis.
    - `summary`: high-level description.
    - `issues`: non-empty array of issues.
    - `metadata`: optional extras (like source file, tokens used).
üí° Think of the schema as our **validation gate**: no matter what OpenAI returns, our system enforces "only responses matching this shape move forward."

### Using the Schema
**`commands/compliance/utils/parseComplianceResponse.ts`
```ts
import { ComplianceAnalysisResponseSchema, ComplianceAnalysisResponse } from "../schemas/complianceAnalysisResponse"

export function parseComplianceResponse(raw: unknown): ComplianceAnalysisResponse {
  const result = ComplianceAnalysisResponseSchema.safeParse(raw)

  if (!result.success) {
    console.error("‚ùå Invalid compliance response:", result.error.format())
    throw new Error("Compliance analysis response failed schema validation")
  }

  return result.data
}
```
#### **2. How `parseComplianceResponse` Uses the Schema**
`parseComplianceResponse` is a **thin wrapper** that:
**1.** Calls `safeParse` with the raw AI response.
**2.** If invalid, logs + throws an error ‚Üí preventing garbage from contaminating downstream systems.
**3.** If valid, returns a strongly typed object (`ComplianceAnalysisResponse`)
This isolates schema validation logic from our CLI command code. That way out CLI remains lean: it doesn't care about Zod internals, only whether it got a valid object or an error.

### Validate AI Responses in `analyzeAuditHandlers.js`
Then in our Analyze Audit Handler, we can validate AI responses like this:
```ts
import { parseComplianceResponse } from "./utils/parseComplianceResponse"

// ...after getting AI response...
const validated = parseComplianceResponse(structured)
console.log("‚úÖ Valid compliance response:", validated)
```
**1. Added import** for `parseComplianceResponse`.
**2. Separated concerns:**
    - `raw` = OpenAI text response.
    - `structured` = parsed JSON.
    - `validated` = schema-checked safe object.
**3. Return `validated`** ‚Üí so our command (`analyzeAuditCommand.js`) only ever receives safe, typed data.

This gives us **type safety, runtime validation, and future portability** when we move into the Cannabis-Compliance backend.

#### **3. How Validation Flows (`analyzeAuditHandlers.js`)**
Inside our compliance command:
**1. Command gets triggered** (via Commander.js or equivalent).
**2.** We call the handler(`analyzeAuditHandler`).
**3.** That handler fetches an AI response.
**4.** Before doing *anything* with the response, we pass it through `parseComplianceResponse`.
**5.**  - If **valid**: continue to display/store/log.
        - If **invalid**: throw ‚Üí CLI shows an error instead of silently swallowing bad output.
This ensures **reliability first**: our command won't ever pipe malformed AI junk into the compliance system.

#### **4. Recap Yesterday's Integration (`analyzeAuditCommand.js` & `analyzeAuditHandlers.js`)**
Lets validate our structure:
- `analyzeAuditCommand.js`
Registers the CLI command. Handles flags, args, and wiring into Commander.
‚Üí Think of this as the **entrypoint glue.**
- `analyzeAuditHandlers.js`
Implements the logic. Deals with calling the AI, interpreting results, and (soon) validating them.
‚Üí Think of this as the **business logic.**

This separation is üëå because:
- Commands stay skinny ‚Üí only know how to register + route.
- Handlers do the heavy lifting.
- Easy to test handlers independently without invoking the CLI framework.

---

### Refactor Prompt so the AI's responses *fit into our system*

#### **1. The Schema is the Contract (Not the Prompt)**
- The **schema** is our **system contract**--the shape our backend guarantees downstream consumers will always see.
- the **prompt** is just one way of *trying* to get the AI to return something that matches that contract.
#### **2. How this Connects to the Starter Schema**
The starter schema was "minimal" because the **prompt at the time** only asked for `summary`, `deviations`, and `mappedGates`.
#### **3. Evolving the Schema**
- Keep `summary` as its own top-level string (this is valuable for humans/regulators and isn't itself an "issue").
- Use `issues` **(aka deviations)** for the detailed compliance problems (id, message, severity, etc.).
- Add `mappedGates` as another top-level array, because this is a classification/taxonomy dimension, not a property of each issue.
#### **4. What This Means for the Prompt**
Now our **prompt** shuold evolve to target this schema:
```vbnet
Respond in strict JSON with keys:
- auditId (string)
- timestamp (ISO 8601)
- analyzedBy ("AI")
- summary (string)
- issues (array of objects with keys: message, severity, regulation, suggestion)
- mappedGates (array of strings)
- metadata (object with optional keys: sourceFile, tokensUsed)
```
This way, the schema and prompt are aligned, and the AI is "nudged" to fill in the fields that our system enforces.
#### **5. Refactored Prompt:**
```js
// 2. Build the prompt
const prompt = `
You are an AI compliance assistant. Review the following audit log entry 
and produce a structured compliance analysis. 

‚ö†Ô∏è Respond ONLY in strict JSON, matching this schema exactly:
{
  "auditId": string,                        // unique audit log ID
  "timestamp": string,                      // ISO 8601 timestamp
  "analyzedBy": "AI",                       // literal value "AI"
  "summary": string,                        // plain-language overview
  "issues": [                               // detected deviations/issues
    {
      "id": string (uuid, optional),
      "message": string,                    // description of the issue
      "severity": "low"|"medium"|"high"|"critical" (optional),
      "regulation": string (optional),      // e.g. OSHA, METRC, etc.
      "suggestion": string (optional)       // AI-generated recommendation
    }
  ],
  "mappedGates": [string],                  // compliance gates like "QA/QC", "OSHA"
  "metadata": {                             // optional info
    "sourceFile": string (optional),
    "tokensUsed": number (optional)
  }
}

Audit Log Data:
${JSON.stringify(auditLog, null, 2)}
`;
```

#### **How This Flows with Validation**
**1. Prompt ensures** the AI is nudged toward the right keys.
**2. Handler parses** and validates using `ComplianceAnalysisResponseSchema`.
**3. Command displays** a guaranteed-safe result.

### Strict Prompt + Schema Only vs Post-Processing fallbacks (Strict vs Resilient)
- **Development phase (Dev Helper sandbox)** ‚Üí we'll go **strict** first. Let validation errors surface, because every failure will teach us how to tweak the prompt and schema alignment.
- **Production phase (Cannabis-Compliance backend)** ‚Üí add **prost-processing fallbacks** to ensure resilience. Regulators, auditors, or users won't tolerate "AI barfed, CLI exploded."
#### Adding **normalization layer** without breaking the "strict-first" workflow
**`commands/compliance/utils/normalizeComplianceResponse.ts`:**
```ts
/**
 * Normalize raw AI JSON output before schema validation.
 * This ensures missing optional fields are filled with safe defaults,
 * reducing spurious validation failures in production.
 *
 * @param {any} raw - Raw object parsed from OpenAI response
 * @returns {any} normalized object ready for schema validation
 */
export function normalizeComplianceResponse(raw) {
  const normalized = { ...raw };

  // Default empty array for gates
  if (!Array.isArray(normalized.mappedGates)) {
    normalized.mappedGates = [];
  }

  // Default empty array for issues
  if (!Array.isArray(normalized.issues)) {
    normalized.issues = [];
  }

  // Metadata object with safe defaults
  if (!normalized.metadata || typeof normalized.metadata !== "object") {
    normalized.metadata = {};
  }

  return normalized;
}
```
#### Updated Handler
```js
import { parseComplianceResponse } from "./utils/parseComplianceResponse.js";
import { normalizeComplianceResponse } from "./utils/normalizeComplianceResponse.js";

export async function handleAnalyzeAudit(filePath) {
  try {
    // ...load audit log, build prompt, call OpenAI...

    const raw = response.choices[0].message.content;
    let structured;

    try {
      structured = JSON.parse(raw);
    } catch (err) {
      throw new Error(`Failed to parse OpenAI response as JSON: ${raw}`);
    }

    // ‚úÖ Normalize before validation (optional in prod)
    const normalized = normalizeComplianceResponse(structured);

    // ‚úÖ Validate against schema
    const validated = parseComplianceResponse(normalized);

    return validated;
  } catch (err) {
    console.error("‚ùå Error in handleAnalyzeAudit:", err.message);
    throw err;
  }
}
```

### Testing
**1. Test Suite 1: Normalization Test** `__tests__/normalizeComplianceResponse.test.ts`
    - Shows how normalization rescues bad AI output (missing fields), so we can run it in our sandbox and see the before/after clearly
    - **What This Test Covers:**
        1. **Missing fields** ‚Üí Normalizer injects defaults (`[]`, `{}`).
        2. **Existing fields** ‚Üí Normalizer doesn't overwrite good data.
        3. **Schema integration** ‚Üí After normalization, the object *always* passes validation.
    - **How to Run**
        - If using Jest or Vitest:
            - ```bash
            npm test __tests__/normalizeComplianceResponse.test.ts
            ```
**2. Test Suite 2: Entire Handler Pipeline** `__tests__/analyzeAuditHandlers.test.ts`
    - Simulates the **entire handler pipeline**, showing how malformed AI responses can still pass safely through normalization + schema validation.
    - **What This Test Demonstrates**
        - **OpenAI mocked** ‚Üí no API calls, just controlled fake data.
        - **Malformed response** (missing `mappedGates`, `metadata`) ‚Üí pipeline still succeeds.
        - **Normalizer** fills gaps.
        - **Schema validation** guarantees structure.
        - **Handler** returns safe, contract-abiding object to the command.
    - **How to Run**
        - ```bash
        npm test __tests__/analyzeAuditHandlers.test.ts
        ```
‚úÖ**Unit Tests for the normalizer *and* integration tests for the handler pipeline.**

#### **Strict mode vs resilient mode toggle**
- **Dev Helper (sandbox)** ‚Üí strict mode on ‚Üí validation fails loudly, teaches us where prompts drift.
- **Cannabis-Compliance (production)** ‚Üí strict mode off ‚Üí normalization applied, system resilient for end users.

#### **How to Toggle Modes**
- **Strict mode (Dev Helper sandbox)**
```bash
STRICT_VALIDATION=true npm test
STRICT_VALIDATION=true node cli.js analyze-audit audit.json
```
- **Resilient mode (Cannabis-Compliance backend / demos)**
```bash
STRICT_VALIDATION=false node cli.js analyze-audit audit.json
```

---

## Wrap Up

### ‚úÖ Today‚Äôs Accomplishments
- Clarified the **role of schema vs. prompt** ‚Üí schema = contract, prompt = instruction.
- Refactored `analyzeAuditHandlers.js` to validate AI output with Zod.
- Upgraded schema alignment to reflect **real system-level fields** (auditId, timestamp, issues, mappedGates, metadata).
- Built a **refactored prompt** that instructs AI to return exactly that shape.
- Added a **normalization layer** for resilience (strict-first now, fallbacks later).
- Outlined **two test suites**:
    1. `normalizeComplianceResponse.test.ts` ‚Üí unit test normalizer.
    2. `analyzeAuditHandlers.test.ts` ‚Üí integration test of full handler pipeline.
- Added an **env flag toggle** (`STRICT_VALIDATION`) to switch strict vs resilient behavior.

### üéØ Tomorrow‚Äôs First Steps
- Implement + run our test suites:
    - **Strict mode** first ‚Üí confirm schema catches malformed AI JSON.
    - **Resilient mode** ‚Üí confirm normalization rescues and schema passes.

- Add a **sample-invalid-response.json** to sanity check CLI behavior.
- Start scaffolding `auditAutomationService.js` & `auditAutomationController.js` in Cannabis-Compliance repo (even just empty methods + TODOs).